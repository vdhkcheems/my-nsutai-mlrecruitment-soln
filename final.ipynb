{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\", index_col=0)\n",
    "train_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>m_1</th>\n",
       "      <th>m_2</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>f1_2</th>\n",
       "      <th>f1_3</th>\n",
       "      <th>f1_4</th>\n",
       "      <th>f1_5</th>\n",
       "      <th>...</th>\n",
       "      <th>mc_2</th>\n",
       "      <th>mc_3</th>\n",
       "      <th>mc_4</th>\n",
       "      <th>mc_5</th>\n",
       "      <th>mc_6</th>\n",
       "      <th>mc_7</th>\n",
       "      <th>mc_8</th>\n",
       "      <th>mc_9</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>radius</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.030832</td>\n",
       "      <td>0.852296</td>\n",
       "      <td>0.296590</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.088141</td>\n",
       "      <td>0.367551</td>\n",
       "      <td>0.409911</td>\n",
       "      <td>0.413748</td>\n",
       "      <td>0.564725</td>\n",
       "      <td>0.809548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.676719</td>\n",
       "      <td>-0.696184</td>\n",
       "      <td>-0.342587</td>\n",
       "      <td>-2.504590</td>\n",
       "      <td>1.587297</td>\n",
       "      <td>-1.664303</td>\n",
       "      <td>0.623092</td>\n",
       "      <td>-0.040765</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>1.335895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.579073</td>\n",
       "      <td>0.726976</td>\n",
       "      <td>0.771023</td>\n",
       "      <td>-0.459303</td>\n",
       "      <td>0.497695</td>\n",
       "      <td>0.955265</td>\n",
       "      <td>0.059640</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.918909</td>\n",
       "      <td>0.350725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737523</td>\n",
       "      <td>-0.640346</td>\n",
       "      <td>2.513343</td>\n",
       "      <td>2.263871</td>\n",
       "      <td>2.137233</td>\n",
       "      <td>-2.977946</td>\n",
       "      <td>0.301570</td>\n",
       "      <td>0.031460</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.927484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.247075</td>\n",
       "      <td>0.939711</td>\n",
       "      <td>0.788165</td>\n",
       "      <td>0.614665</td>\n",
       "      <td>0.904854</td>\n",
       "      <td>0.759019</td>\n",
       "      <td>0.473408</td>\n",
       "      <td>0.874605</td>\n",
       "      <td>0.184836</td>\n",
       "      <td>0.511990</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.408861</td>\n",
       "      <td>-3.033708</td>\n",
       "      <td>0.405962</td>\n",
       "      <td>-0.667211</td>\n",
       "      <td>-0.082856</td>\n",
       "      <td>1.127740</td>\n",
       "      <td>-1.569627</td>\n",
       "      <td>-0.067948</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>0.969543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.484133</td>\n",
       "      <td>-1.058733</td>\n",
       "      <td>-0.255312</td>\n",
       "      <td>0.945397</td>\n",
       "      <td>0.386055</td>\n",
       "      <td>0.562933</td>\n",
       "      <td>0.054297</td>\n",
       "      <td>0.775500</td>\n",
       "      <td>0.976286</td>\n",
       "      <td>0.830746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202319</td>\n",
       "      <td>0.413482</td>\n",
       "      <td>1.160030</td>\n",
       "      <td>-2.419326</td>\n",
       "      <td>0.938661</td>\n",
       "      <td>-4.280382</td>\n",
       "      <td>-0.220232</td>\n",
       "      <td>1.016019</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>1.166106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.753558</td>\n",
       "      <td>-0.477875</td>\n",
       "      <td>1.460296</td>\n",
       "      <td>-0.377830</td>\n",
       "      <td>0.025013</td>\n",
       "      <td>0.685838</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.357408</td>\n",
       "      <td>0.956214</td>\n",
       "      <td>0.913230</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.048132</td>\n",
       "      <td>-0.564583</td>\n",
       "      <td>1.017124</td>\n",
       "      <td>-0.043037</td>\n",
       "      <td>1.024410</td>\n",
       "      <td>-0.338675</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.954277</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.893281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         c_1       c_2       m_1       m_2      f1_0      f1_1      f1_2  \\\n",
       "ID                                                                         \n",
       "0  -1.030832  0.852296  0.296590  0.961859  0.088141  0.367551  0.409911   \n",
       "1  -0.579073  0.726976  0.771023 -0.459303  0.497695  0.955265  0.059640   \n",
       "2   0.247075  0.939711  0.788165  0.614665  0.904854  0.759019  0.473408   \n",
       "3  -0.484133 -1.058733 -0.255312  0.945397  0.386055  0.562933  0.054297   \n",
       "4  -0.753558 -0.477875  1.460296 -0.377830  0.025013  0.685838  0.602755   \n",
       "\n",
       "        f1_3      f1_4      f1_5  ...      mc_2      mc_3      mc_4      mc_5  \\\n",
       "ID                                ...                                           \n",
       "0   0.413748  0.564725  0.809548  ... -0.676719 -0.696184 -0.342587 -2.504590   \n",
       "1   0.003631  0.918909  0.350725  ...  0.737523 -0.640346  2.513343  2.263871   \n",
       "2   0.874605  0.184836  0.511990  ... -3.408861 -3.033708  0.405962 -0.667211   \n",
       "3   0.775500  0.976286  0.830746  ...  0.202319  0.413482  1.160030 -2.419326   \n",
       "4   0.357408  0.956214  0.913230  ... -1.048132 -0.564583  1.017124 -0.043037   \n",
       "\n",
       "        mc_6      mc_7      mc_8      mc_9    TARGET    radius  \n",
       "ID                                                              \n",
       "0   1.587297 -1.664303  0.623092 -0.040765 -0.002541  1.335895  \n",
       "1   2.137233 -2.977946  0.301570  0.031460  0.006602  0.927484  \n",
       "2  -0.082856  1.127740 -1.569627 -0.067948 -0.006384  0.969543  \n",
       "3   0.938661 -4.280382 -0.220232  1.016019 -0.001549  1.166106  \n",
       "4   1.024410 -0.338675 -0.084735  0.954277  0.001167  0.893281  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_point = df[['c_1', 'c_2']].mean()\n",
    "df['radius'] = np.sqrt((df['c_1'] - center_point['c_1'])**2 + (df['c_2'] - center_point['c_2'])**2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agc = AgglomerativeClustering(linkage = 'single')\n",
    "agc.fit(df[['m_1', 'm_2']])\n",
    "agc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7680eb44d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['m_1'], df['m_2'], c = agc.labels_, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>m_1</th>\n",
       "      <th>m_2</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>f1_2</th>\n",
       "      <th>f1_3</th>\n",
       "      <th>f1_4</th>\n",
       "      <th>f1_5</th>\n",
       "      <th>...</th>\n",
       "      <th>mc_3</th>\n",
       "      <th>mc_4</th>\n",
       "      <th>mc_5</th>\n",
       "      <th>mc_6</th>\n",
       "      <th>mc_7</th>\n",
       "      <th>mc_8</th>\n",
       "      <th>mc_9</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>radius</th>\n",
       "      <th>m_cluster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.030832</td>\n",
       "      <td>0.852296</td>\n",
       "      <td>0.296590</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.088141</td>\n",
       "      <td>0.367551</td>\n",
       "      <td>0.409911</td>\n",
       "      <td>0.413748</td>\n",
       "      <td>0.564725</td>\n",
       "      <td>0.809548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.696184</td>\n",
       "      <td>-0.342587</td>\n",
       "      <td>-2.504590</td>\n",
       "      <td>1.587297</td>\n",
       "      <td>-1.664303</td>\n",
       "      <td>0.623092</td>\n",
       "      <td>-0.040765</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>1.335895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.579073</td>\n",
       "      <td>0.726976</td>\n",
       "      <td>0.771023</td>\n",
       "      <td>-0.459303</td>\n",
       "      <td>0.497695</td>\n",
       "      <td>0.955265</td>\n",
       "      <td>0.059640</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.918909</td>\n",
       "      <td>0.350725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640346</td>\n",
       "      <td>2.513343</td>\n",
       "      <td>2.263871</td>\n",
       "      <td>2.137233</td>\n",
       "      <td>-2.977946</td>\n",
       "      <td>0.301570</td>\n",
       "      <td>0.031460</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.927484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.247075</td>\n",
       "      <td>0.939711</td>\n",
       "      <td>0.788165</td>\n",
       "      <td>0.614665</td>\n",
       "      <td>0.904854</td>\n",
       "      <td>0.759019</td>\n",
       "      <td>0.473408</td>\n",
       "      <td>0.874605</td>\n",
       "      <td>0.184836</td>\n",
       "      <td>0.511990</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.033708</td>\n",
       "      <td>0.405962</td>\n",
       "      <td>-0.667211</td>\n",
       "      <td>-0.082856</td>\n",
       "      <td>1.127740</td>\n",
       "      <td>-1.569627</td>\n",
       "      <td>-0.067948</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>0.969543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.484133</td>\n",
       "      <td>-1.058733</td>\n",
       "      <td>-0.255312</td>\n",
       "      <td>0.945397</td>\n",
       "      <td>0.386055</td>\n",
       "      <td>0.562933</td>\n",
       "      <td>0.054297</td>\n",
       "      <td>0.775500</td>\n",
       "      <td>0.976286</td>\n",
       "      <td>0.830746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413482</td>\n",
       "      <td>1.160030</td>\n",
       "      <td>-2.419326</td>\n",
       "      <td>0.938661</td>\n",
       "      <td>-4.280382</td>\n",
       "      <td>-0.220232</td>\n",
       "      <td>1.016019</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>1.166106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.753558</td>\n",
       "      <td>-0.477875</td>\n",
       "      <td>1.460296</td>\n",
       "      <td>-0.377830</td>\n",
       "      <td>0.025013</td>\n",
       "      <td>0.685838</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.357408</td>\n",
       "      <td>0.956214</td>\n",
       "      <td>0.913230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.564583</td>\n",
       "      <td>1.017124</td>\n",
       "      <td>-0.043037</td>\n",
       "      <td>1.024410</td>\n",
       "      <td>-0.338675</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.954277</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.893281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         c_1       c_2       m_1       m_2      f1_0      f1_1      f1_2  \\\n",
       "ID                                                                         \n",
       "0  -1.030832  0.852296  0.296590  0.961859  0.088141  0.367551  0.409911   \n",
       "1  -0.579073  0.726976  0.771023 -0.459303  0.497695  0.955265  0.059640   \n",
       "2   0.247075  0.939711  0.788165  0.614665  0.904854  0.759019  0.473408   \n",
       "3  -0.484133 -1.058733 -0.255312  0.945397  0.386055  0.562933  0.054297   \n",
       "4  -0.753558 -0.477875  1.460296 -0.377830  0.025013  0.685838  0.602755   \n",
       "\n",
       "        f1_3      f1_4      f1_5  ...      mc_3      mc_4      mc_5      mc_6  \\\n",
       "ID                                ...                                           \n",
       "0   0.413748  0.564725  0.809548  ... -0.696184 -0.342587 -2.504590  1.587297   \n",
       "1   0.003631  0.918909  0.350725  ... -0.640346  2.513343  2.263871  2.137233   \n",
       "2   0.874605  0.184836  0.511990  ... -3.033708  0.405962 -0.667211 -0.082856   \n",
       "3   0.775500  0.976286  0.830746  ...  0.413482  1.160030 -2.419326  0.938661   \n",
       "4   0.357408  0.956214  0.913230  ... -0.564583  1.017124 -0.043037  1.024410   \n",
       "\n",
       "        mc_7      mc_8      mc_9    TARGET    radius  m_cluster  \n",
       "ID                                                               \n",
       "0  -1.664303  0.623092 -0.040765 -0.002541  1.335895          0  \n",
       "1  -2.977946  0.301570  0.031460  0.006602  0.927484          1  \n",
       "2   1.127740 -1.569627 -0.067948 -0.006384  0.969543          0  \n",
       "3  -4.280382 -0.220232  1.016019 -0.001549  1.166106          0  \n",
       "4  -0.338675 -0.084735  0.954277  0.001167  0.893281          1  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['m_cluster'] = agc.labels_\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>m_1</th>\n",
       "      <th>m_2</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>f1_2</th>\n",
       "      <th>f1_3</th>\n",
       "      <th>f1_4</th>\n",
       "      <th>f1_5</th>\n",
       "      <th>...</th>\n",
       "      <th>mc_2</th>\n",
       "      <th>mc_3</th>\n",
       "      <th>mc_4</th>\n",
       "      <th>mc_5</th>\n",
       "      <th>mc_6</th>\n",
       "      <th>mc_7</th>\n",
       "      <th>mc_8</th>\n",
       "      <th>mc_9</th>\n",
       "      <th>radius</th>\n",
       "      <th>m_cluster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453135</td>\n",
       "      <td>0.631948</td>\n",
       "      <td>0.946726</td>\n",
       "      <td>0.284150</td>\n",
       "      <td>0.390982</td>\n",
       "      <td>0.993040</td>\n",
       "      <td>0.858435</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>0.016954</td>\n",
       "      <td>0.440202</td>\n",
       "      <td>...</td>\n",
       "      <td>2.606183</td>\n",
       "      <td>-0.385630</td>\n",
       "      <td>0.306463</td>\n",
       "      <td>-3.454732</td>\n",
       "      <td>0.659152</td>\n",
       "      <td>-1.049579</td>\n",
       "      <td>-1.646701</td>\n",
       "      <td>-0.755683</td>\n",
       "      <td>0.775952</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.446105</td>\n",
       "      <td>-0.206687</td>\n",
       "      <td>1.225122</td>\n",
       "      <td>-0.472673</td>\n",
       "      <td>0.590084</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.121248</td>\n",
       "      <td>0.085474</td>\n",
       "      <td>0.575945</td>\n",
       "      <td>0.362193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059189</td>\n",
       "      <td>0.531227</td>\n",
       "      <td>-1.456298</td>\n",
       "      <td>0.197297</td>\n",
       "      <td>-3.631915</td>\n",
       "      <td>3.482724</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>-1.253722</td>\n",
       "      <td>0.492359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.929768</td>\n",
       "      <td>-0.236726</td>\n",
       "      <td>-0.211991</td>\n",
       "      <td>0.983287</td>\n",
       "      <td>0.255440</td>\n",
       "      <td>0.997218</td>\n",
       "      <td>0.185513</td>\n",
       "      <td>0.495543</td>\n",
       "      <td>0.409448</td>\n",
       "      <td>0.371395</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130395</td>\n",
       "      <td>-0.979486</td>\n",
       "      <td>-2.604229</td>\n",
       "      <td>-0.685552</td>\n",
       "      <td>-0.519991</td>\n",
       "      <td>3.175169</td>\n",
       "      <td>0.125237</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>0.959719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.087014</td>\n",
       "      <td>0.474569</td>\n",
       "      <td>0.971476</td>\n",
       "      <td>0.265174</td>\n",
       "      <td>0.904335</td>\n",
       "      <td>0.440854</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>0.443708</td>\n",
       "      <td>0.607987</td>\n",
       "      <td>0.893845</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.059669</td>\n",
       "      <td>0.314280</td>\n",
       "      <td>3.216311</td>\n",
       "      <td>1.777331</td>\n",
       "      <td>2.378461</td>\n",
       "      <td>-4.346591</td>\n",
       "      <td>-0.402699</td>\n",
       "      <td>1.677564</td>\n",
       "      <td>1.185449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.286852</td>\n",
       "      <td>-0.550938</td>\n",
       "      <td>-0.155366</td>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.517972</td>\n",
       "      <td>0.875242</td>\n",
       "      <td>0.706647</td>\n",
       "      <td>0.114120</td>\n",
       "      <td>0.363096</td>\n",
       "      <td>0.052569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.873623</td>\n",
       "      <td>-0.137316</td>\n",
       "      <td>0.464656</td>\n",
       "      <td>-2.001316</td>\n",
       "      <td>1.723290</td>\n",
       "      <td>-0.734778</td>\n",
       "      <td>1.078520</td>\n",
       "      <td>-1.471971</td>\n",
       "      <td>1.400461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         c_1       c_2       m_1       m_2      f1_0      f1_1      f1_2  \\\n",
       "ID                                                                         \n",
       "0   0.453135  0.631948  0.946726  0.284150  0.390982  0.993040  0.858435   \n",
       "1  -0.446105 -0.206687  1.225122 -0.472673  0.590084  0.682606  0.121248   \n",
       "2  -0.929768 -0.236726 -0.211991  0.983287  0.255440  0.997218  0.185513   \n",
       "3   1.087014  0.474569  0.971476  0.265174  0.904335  0.440854  0.601088   \n",
       "4  -1.286852 -0.550938 -0.155366  0.980676  0.517972  0.875242  0.706647   \n",
       "\n",
       "        f1_3      f1_4      f1_5  ...      mc_2      mc_3      mc_4      mc_5  \\\n",
       "ID                                ...                                           \n",
       "0   0.025059  0.016954  0.440202  ...  2.606183 -0.385630  0.306463 -3.454732   \n",
       "1   0.085474  0.575945  0.362193  ... -0.059189  0.531227 -1.456298  0.197297   \n",
       "2   0.495543  0.409448  0.371395  ...  1.130395 -0.979486 -2.604229 -0.685552   \n",
       "3   0.443708  0.607987  0.893845  ... -1.059669  0.314280  3.216311  1.777331   \n",
       "4   0.114120  0.363096  0.052569  ...  1.873623 -0.137316  0.464656 -2.001316   \n",
       "\n",
       "        mc_6      mc_7      mc_8      mc_9    radius  m_cluster  \n",
       "ID                                                               \n",
       "0   0.659152 -1.049579 -1.646701 -0.755683  0.775952          0  \n",
       "1  -3.631915  3.482724  0.220615 -1.253722  0.492359          1  \n",
       "2  -0.519991  3.175169  0.125237  0.999117  0.959719          0  \n",
       "3   2.378461 -4.346591 -0.402699  1.677564  1.185449          0  \n",
       "4   1.723290 -0.734778  1.078520 -1.471971  1.400461          0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['radius'] = np.sqrt((test['c_1'] - center_point['c_1'])**2 + (test['c_2'] - center_point['c_2'])**2)\n",
    "agc = AgglomerativeClustering(linkage = 'single')\n",
    "agc.fit(test[['m_1', 'm_2']])\n",
    "test['m_cluster'] = agc.labels_\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['TARGET'])\n",
    "y = df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming 'X' is your feature data (NumPy array or Pandas DataFrame)\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming 'X' is your feature data (NumPy array or Pandas DataFrame)\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "test = scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000025\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32666, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 1 RMSE: 0.00110223525524395\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32666, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000036\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 2 RMSE: 0.000921298263617872\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32666, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000034\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 3 RMSE: 0.001103519295334043\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32666, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000024\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 4 RMSE: 0.0012625977047617646\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32666, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000023\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 5 RMSE: 0.0013269198473461088\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000026\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 6 RMSE: 0.001121136053677142\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000017\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 7 RMSE: 0.0011579167966011817\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000009\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 8 RMSE: 0.0014118760924769467\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 9 RMSE: 0.0012372677593511427\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000010\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 10 RMSE: 0.0013144076004889857\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 11 RMSE: 0.0010838372480166476\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000008\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 12 RMSE: 0.0010789429560621763\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000042\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 13 RMSE: 0.0015443581763225883\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000045\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 14 RMSE: 0.0009723489265301547\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10967\n",
      "[LightGBM] [Info] Number of data points in the train set: 32667, number of used features: 44\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.000030\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Fold 15 RMSE: 0.0013755912811587066\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[-0.00154468  0.00237889 -0.00256642 ... -0.00161334 -0.00432747\n",
      " -0.00056841]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "n_folds = 15\n",
    "weights = []\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "train_data = X\n",
    "train_target = y\n",
    "\n",
    "estimator = LGBMRegressor(boosting_type='goss', num_leaves=60, learning_rate=0.1,max_depth=12, n_estimators=400)\n",
    "selector = RFE(estimator, n_features_to_select=44, step=1)\n",
    "selector.fit(train_data, train_target)\n",
    "rfe_train_data = train_data.iloc[:, selector.support_]\n",
    "test_data = test.iloc[:, selector.support_]\n",
    "test_preds = np.zeros((test_data.shape[0],))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "    X_train, y_train = rfe_train_data.iloc[train_idx], train_target[train_idx]\n",
    "    X_val, y_val = rfe_train_data.iloc[val_idx], train_target[val_idx]\n",
    "\n",
    "    model = LGBMRegressor(boosting_type='goss', num_leaves=60, learning_rate=0.1,max_depth=12, n_estimators=400)\n",
    "  # Use SVR instead of Linear Regression\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    val_preds = model.predict(X_val)\n",
    "    fold_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "    weights.append(1 / fold_rmse)\n",
    "    print(f'Fold {fold+1} RMSE: {fold_rmse}')\n",
    "    test_preds += model.predict(test_data) * weights[fold]\n",
    "test_preds /= sum(weights)\n",
    "\n",
    "print(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['ID', 'TARGET'])\n",
    "result['ID'] = test.index\n",
    "result['TARGET'] = test_preds\n",
    "result.to_csv('data/submissionlgbm3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
